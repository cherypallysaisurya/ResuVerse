{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cherypallysaisurya/ResuVerse/blob/main/dataextraction%26matchingscore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5v626XGPaUP",
        "outputId": "ca8618ac-cb1f-49ed-e5ae-843c776e1db8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytesseract, PyMuPDF, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed PyMuPDF-1.25.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "pip install PyMuPDF pytesseract opencv-python-headless Pillow transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDNInHYeO2VG"
      },
      "source": [
        "###code for extration###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 437,
          "referenced_widgets": [
            "7c70df796cbb46f6b1a812eb78235a46",
            "7ac26812a35d4ee399ee073d8d2dd4b0",
            "52d2b4d635a4488382814954446298da",
            "63b48a48b76c4ff3af173099964d7858",
            "d583045e91af441dbf383bfc4f4829af",
            "d0d02bffbe274006bbc406de9643b566"
          ]
        },
        "id": "K6dFDCcSORzX",
        "outputId": "3c289fe2-5bc4-4f30-eaff-0a450b2a5203"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c70df796cbb46f6b1a812eb78235a46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ac26812a35d4ee399ee073d8d2dd4b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52d2b4d635a4488382814954446298da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63b48a48b76c4ff3af173099964d7858",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d583045e91af441dbf383bfc4f4829af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0d02bffbe274006bbc406de9643b566",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import logging\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "REGEX_NEWLINES = re.compile(r'\\n{3,}')\n",
        "REGEX_PAGENUM = re.compile(r'Page \\d+ of \\d+')\n",
        "REGEX_MULTISPACE = re.compile(r' {2,}')\n",
        "REGEX_BULLET = re.compile(r'•\\s*')\n",
        "\n",
        "section_synonyms = {\n",
        "    \"scope of work\": [\n",
        "        \"scope of work\", \"services required\", \"project scope\", \"exhibit a\",\n",
        "        \"description and standards\", \"scope\", \"statement of work\",\n",
        "        \"contract scope\", \"work requirements\", \"contractor responsibilities\",\n",
        "        \"technical requirements\", \"deliverables\", \"performance requirements\"\n",
        "    ],\n",
        "    \"background\": [\n",
        "        \"background\", \"project background\", \"general information\",\n",
        "        \"project overview\", \"project description\", \"introduction\",\n",
        "        \"program background\", \"overview\", \"agency background\",\n",
        "        \"purpose\", \"executive summary\", \"agency needs\", \"situation overview\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "section_keywords = {\n",
        "    \"scope of work\": [\n",
        "        \"provider\", \"contractor\", \"responsibilities\", \"services\", \"requirements\",\n",
        "        \"shall\", \"must\", \"deliverable\", \"perform\", \"duty\", \"obligation\",\n",
        "        \"sourcing\", \"screening\", \"reimbursement\", \"insurance\", \"licensure\",\n",
        "        \"travel\", \"lodging\", \"documentation\", \"EHR\"\n",
        "    ],\n",
        "    \"background\": [\n",
        "        \"mission\", \"purpose\", \"organization\", \"department\", \"agency\",\n",
        "        \"established\", \"serves\", \"location\", \"region\", \"operation\",\n",
        "        \"history\", \"overview\", \"goal\", \"objective\", \"population\", \"need\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        doc = fitz.open(file_path)\n",
        "        full_text = \"\"\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc[page_num]\n",
        "            blocks = page.get_text(\"blocks\")\n",
        "            page_text = \"\\n\\n\".join(block[4].strip() for block in blocks if block[4].strip())\n",
        "            full_text += f\"\\n--- PAGE {page_num + 1} ---\\n{page_text}\"\n",
        "        logger.info(f\"PDF extraction for {file_path} completed in {time.time() - start_time:.2f} sec\")\n",
        "        return full_text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting text from {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    text = REGEX_NEWLINES.sub(\"\\n\\n\", text)\n",
        "    text = REGEX_PAGENUM.sub(\"\", text)\n",
        "    text = REGEX_MULTISPACE.sub(\" \", text)\n",
        "    text = REGEX_BULLET.sub(\"• \", text)\n",
        "    text = re.sub(r'\\n--- PAGE \\d+ ---\\n', '\\n\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "def identify_section_markers(text):\n",
        "    patterns = [\n",
        "        r'(?:(?:^|\\n)(?:SECTION|Section)[\\s:.]*[0-9A-Z.-]+\\s+([A-Z][A-Za-z\\s]+))',\n",
        "        r'(?:(?:^|\\n)([A-Z][A-Z\\s]{2,})(?:$|\\n))',\n",
        "        r'(?:(?:^|\\n)(\\d+(?:\\.\\d+)*[\\s.]+[A-Z][A-Za-z\\s]+)(?:$|\\n))',\n",
        "        r'(?:(?:^|\\n)([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,3})(?:$|\\n))'\n",
        "    ]\n",
        "\n",
        "    section_markers = set()\n",
        "    for pattern in patterns:\n",
        "        matches = re.finditer(pattern, text)\n",
        "        for match in matches:\n",
        "            header = match.group(1) if match.groups() else \"\"\n",
        "            if header and len(header.strip()) > 3:\n",
        "                section_markers.add(header.strip().lower())\n",
        "\n",
        "    return section_markers\n",
        "\n",
        "def extract_section(text, section_name, identified_markers=None):\n",
        "    base_name = section_name.lower()\n",
        "    confidence = 0.0\n",
        "    patterns = [\n",
        "        re.compile(rf\"(?:^|\\n)(?:SECTION|Section)[\\s:.]*[0-9A-Z.-]+\\s+.*{re.escape(base_name)}[^\\n]*\\n+(.*?)(?=\\n(?:SECTION|Section)[\\s:.]*[0-9A-Z.-]+\\s+|$)\", re.IGNORECASE | re.DOTALL),\n",
        "        re.compile(rf\"(?:^|\\n)\\d+(?:\\.\\d+)*\\s+{re.escape(base_name)}[^\\n]*\\n+(.*?)(?=\\n\\d+(?:\\.\\d+)*\\s+|$)\", re.IGNORECASE | re.DOTALL),\n",
        "        re.compile(rf\"(?:^|\\n){re.escape(base_name.upper())}[^\\n]*\\n+(.*?)(?=\\n[A-Z][A-Z\\s]+(?:$|\\n)|$)\", re.IGNORECASE | re.DOTALL),\n",
        "        re.compile(rf\"(?:^|\\n){re.escape(base_name.title())}[^\\n]*\\n+(.*?)(?=\\n[A-Z][a-z]+\\s+[A-Z][a-z]+(?:$|\\n)|$)\", re.IGNORECASE | re.DOTALL)\n",
        "    ]\n",
        "\n",
        "    if ' of ' in base_name:\n",
        "        parts = base_name.split(' of ')\n",
        "        if len(parts) == 2:\n",
        "            of_pattern = re.compile(rf\"(?:^|\\n){re.escape(parts[0])}\\s+of\\s+{re.escape(parts[1])}[^\\n]*\\n+(.*?)(?=\\n[A-Z][\\w\\s]+|$)\", re.IGNORECASE | re.DOTALL)\n",
        "            patterns.insert(0, of_pattern)\n",
        "\n",
        "    best_match = None\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = pattern.search(text)\n",
        "        if match:\n",
        "            current_match = match.group(1).strip()\n",
        "            current_confidence = min(1.0, len(current_match.split()) / 500) * 0.5\n",
        "\n",
        "            context_start = max(0, match.start() - 100)\n",
        "            context = text[context_start:match.start()]\n",
        "            if re.search(rf\"\\b{re.escape(base_name)}\\b\", context, re.IGNORECASE):\n",
        "                current_confidence += 0.3\n",
        "\n",
        "            if base_name in section_keywords:\n",
        "                key_terms = section_keywords[base_name]\n",
        "                term_matches = sum(1 for term in key_terms if term.lower() in current_match.lower())\n",
        "                current_confidence += min(0.2, term_matches * 0.02)\n",
        "\n",
        "            word_count = len(current_match.split())\n",
        "            if word_count > 1500:\n",
        "                possible_endpoints = re.finditer(r'\\n\\s*\\n[A-Z]', current_match)\n",
        "                positions = [m.start() for m in possible_endpoints]\n",
        "                if positions and positions[0] > 300:\n",
        "                    current_match = current_match[:positions[0]]\n",
        "                    current_confidence = min(current_confidence + 0.1, 1.0)\n",
        "\n",
        "            if current_confidence > confidence:\n",
        "                confidence = current_confidence\n",
        "                best_match = current_match\n",
        "\n",
        "    if best_match is None and identified_markers:\n",
        "        section_words = set(base_name.split())\n",
        "        best_marker = None\n",
        "        best_overlap = 0\n",
        "\n",
        "        for marker in identified_markers:\n",
        "            marker_words = set(marker.split())\n",
        "            overlap = len(section_words.intersection(marker_words))\n",
        "            if overlap > best_overlap:\n",
        "                best_overlap = overlap\n",
        "                best_marker = marker\n",
        "\n",
        "        if best_marker and best_overlap >= 1:\n",
        "            marker_pattern = re.compile(rf\"(?:^|\\n){re.escape(best_marker)}[^\\n]*\\n+(.*?)(?=\\n[A-Z][\\w\\s]+(?:$|\\n)|$)\", re.IGNORECASE | re.DOTALL)\n",
        "            match = marker_pattern.search(text)\n",
        "            if match:\n",
        "                best_match = match.group(1).strip()\n",
        "                confidence = 0.3 + (best_overlap / len(section_words)) * 0.3\n",
        "\n",
        "    if best_match is None:\n",
        "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "        relevant_paragraphs = []\n",
        "\n",
        "        for para in paragraphs:\n",
        "            if base_name in section_keywords:\n",
        "                keyword_count = sum(1 for kw in section_keywords[base_name] if kw.lower() in para.lower())\n",
        "                if keyword_count >= 3 or re.search(rf\"\\b{re.escape(base_name)}\\b\", para, re.IGNORECASE):\n",
        "                    relevant_paragraphs.append(para)\n",
        "\n",
        "        if relevant_paragraphs:\n",
        "            best_match = \"\\n\\n\".join(relevant_paragraphs[:5])\n",
        "            confidence = 0.3\n",
        "\n",
        "    if best_match:\n",
        "        return best_match, confidence\n",
        "    else:\n",
        "        return f\"No '{section_name.title()}' content found.\", 0.0\n",
        "\n",
        "def extract_section_by_synonyms(text, canonical_section, markers):\n",
        "    synonyms = section_synonyms.get(canonical_section, [canonical_section])\n",
        "    best_text = None\n",
        "    best_conf = 0.0\n",
        "\n",
        "    for syn in synonyms:\n",
        "        extracted_text, conf = extract_section(text, syn, markers)\n",
        "        if conf > best_conf and not extracted_text.startswith(\"No '\"):\n",
        "            best_conf = conf\n",
        "            best_text = extracted_text\n",
        "\n",
        "    if best_text:\n",
        "        return best_text, best_conf\n",
        "    else:\n",
        "        return f\"No '{canonical_section.title()}' content found.\", 0.0\n",
        "\n",
        "def preprocess_for_summarization(text):\n",
        "    text = re.sub(r'\\s*\\n\\s*\\n\\s*', '\\n\\n', text)\n",
        "    text = re.sub(r'(?<=\\n)[•*-]\\s*', '• ', text)\n",
        "    text = re.sub(r'(?<=\\n)\\s*\\d+[\\.)]\\s+', lambda m: f\"{m.group().strip()} \", text)\n",
        "    text = re.sub(r'(?<=\\n)\\s*\\d+\\s*(?=\\n)', '', text)\n",
        "    text = re.sub(r'(?<=\\n).*confidential.*(?=\\n)', '', text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "def chunk_text_intelligently(text, max_words=300):\n",
        "    subsection_pattern = r'\\n\\s*\\n(?:\\d+\\.\\d+|[A-Z]\\.\\d+|[a-z]\\)|\\([a-z]\\))\\s+'\n",
        "    subsections = re.split(subsection_pattern, text)\n",
        "\n",
        "    if len(subsections) > 1:\n",
        "        chunks = []\n",
        "        for subsec in subsections:\n",
        "            subsec = subsec.strip()\n",
        "            if not subsec:\n",
        "                continue\n",
        "\n",
        "            if len(subsec.split()) > max_words:\n",
        "                chunks.extend(split_by_sentences(subsec, max_words))\n",
        "            else:\n",
        "                chunks.append(subsec)\n",
        "        return chunks\n",
        "    else:\n",
        "        return split_by_sentences(text, max_words)\n",
        "\n",
        "def split_by_sentences(text, max_words=300):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_word_count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_words = len(sentence.split())\n",
        "        if sentence_words > max_words:\n",
        "            if current_chunk:\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = []\n",
        "                current_word_count = 0\n",
        "\n",
        "            parts = re.split(r'[;:,] (?=[A-Za-z])', sentence)\n",
        "            for part in parts:\n",
        "                part_words = len(part.split())\n",
        "                if part_words > max_words/2:\n",
        "                    chunks.append(part)\n",
        "                else:\n",
        "                    if current_word_count + part_words > max_words and current_chunk:\n",
        "                        chunks.append(\" \".join(current_chunk))\n",
        "                        current_chunk = [part]\n",
        "                        current_word_count = part_words\n",
        "                    else:\n",
        "                        current_chunk.append(part)\n",
        "                        current_word_count += part_words\n",
        "        elif current_word_count + sentence_words > max_words and current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [sentence]\n",
        "            current_word_count = sentence_words\n",
        "        else:\n",
        "            current_chunk.append(sentence)\n",
        "            current_word_count += sentence_words\n",
        "\n",
        "            if current_word_count >= max_words:\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = []\n",
        "                current_word_count = 0\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def generate_section_specific_prompt(section_name, chunk, chunk_info):\n",
        "    if section_name.lower() == \"scope of work\":\n",
        "        prompt = (\n",
        "            f\"Summarize the following text extracted from the 'Scope of Work' section of an RFP. \"\n",
        "            f\"Provide a clear and concise summary that outlines the contractor's responsibilities, including provider sourcing, screening, reimbursement, \"\n",
        "            f\"insurance, licensure verification, travel/lodging management, and EHR documentation. \"\n",
        "            f\"Do not include extra commentary. Text to summarize {chunk_info}:\\n\\n{chunk}\"\n",
        "        )\n",
        "    elif section_name.lower() == \"background\":\n",
        "        prompt = (\n",
        "            f\"Summarize the following text extracted from the 'Project Background' section of an RFP. \"\n",
        "            f\"Provide a clear and concise summary that highlights the department's mission, purpose, geographic context, and operational details. \"\n",
        "            f\"Do not include extra commentary. Text to summarize {chunk_info}:\\n\\n{chunk}\"\n",
        "        )\n",
        "    else:\n",
        "        prompt = (\n",
        "            f\"Summarize the following text from an RFP. Provide a concise summary of key points. \"\n",
        "            f\"Text to summarize {chunk_info}:\\n\\n{chunk}\"\n",
        "        )\n",
        "    return prompt\n",
        "\n",
        "def generate_final_summary_prompt(section_name, combined_summaries, confidence):\n",
        "    note = \"\"\n",
        "    if confidence < 0.5:\n",
        "        note = \" Note: The extracted content may be incomplete.\"\n",
        "\n",
        "    if section_name.lower() == \"scope of work\":\n",
        "        prompt = (\n",
        "            f\"Refine and consolidate the following texts into a final, clear, and detailed summary of the 'Scope of Work' section. \"\n",
        "            f\"Focus exclusively on contractor responsibilities including provider sourcing, screening, reimbursement, insurance, licensure verification, \"\n",
        "            f\"travel/lodging management, and EHR documentation. Output only the final summary, formatted as a coherent paragraph followed by 4-7 bullet points of key responsibilities. {note}\\n\\nCombined Text:\\n{combined_summaries}\"\n",
        "        )\n",
        "    elif section_name.lower() == \"background\":\n",
        "        prompt = (\n",
        "            f\"Refine and consolidate the following texts into a final, clear, and comprehensive summary of the 'Project Background' section. \"\n",
        "            f\"Focus exclusively on the department's mission, purpose, geographic context, and operational details. Output only the final summary, formatted as a coherent paragraph followed by 3-5 bullet points of key organizational details. {note}\\n\\nCombined Text:\\n{combined_summaries}\"\n",
        "        )\n",
        "    else:\n",
        "        prompt = (\n",
        "            f\"Refine and consolidate the following texts into a final summary. \"\n",
        "            f\"Output only the final summary. {note}\\n\\nCombined Text:\\n{combined_summaries}\"\n",
        "        )\n",
        "    return prompt\n",
        "\n",
        "def remove_redundancy(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    if len(sentences) <= 5:\n",
        "        return text\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    try:\n",
        "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "        similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "        redundant_indices = set()\n",
        "        for i in range(len(sentences)):\n",
        "            if i in redundant_indices:\n",
        "                continue\n",
        "            for j in range(i+1, len(sentences)):\n",
        "                if similarity_matrix[i, j] > 0.7:\n",
        "                    redundant_indices.add(j)\n",
        "\n",
        "        unique_sentences = [s for idx, s in enumerate(sentences) if idx not in redundant_indices]\n",
        "        return \" \".join(unique_sentences)\n",
        "    except:\n",
        "        return text\n",
        "\n",
        "def verify_section_content(summary, section_name):\n",
        "    if section_name.lower() == \"scope of work\":\n",
        "        key_topics = {\n",
        "            \"provider sourcing\": [\"provider\", \"sourcing\", \"recruit\", \"acquire\"],\n",
        "            \"screening\": [\"screen\", \"evaluate\", \"assess\", \"review\"],\n",
        "            \"reimbursement\": [\"reimbursement\", \"payment\", \"compensation\", \"fee\"],\n",
        "            \"insurance\": [\"insurance\", \"coverage\", \"liability\"],\n",
        "            \"licensure\": [\"license\", \"certification\", \"credential\"],\n",
        "            \"travel/lodging\": [\"travel\", \"lodging\", \"accommodation\", \"housing\"],\n",
        "            \"EHR documentation\": [\"EHR\", \"documentation\", \"record\", \"chart\"]\n",
        "        }\n",
        "    elif section_name.lower() == \"background\":\n",
        "        key_topics = {\n",
        "            \"mission\": [\"mission\", \"purpose\", \"goal\", \"objective\"],\n",
        "            \"organizational purpose\": [\"organization\", \"department\", \"agency\", \"authority\"],\n",
        "            \"geographic context\": [\"location\", \"region\", \"area\", \"geographic\", \"jurisdiction\"],\n",
        "            \"operational details\": [\"operation\", \"process\", \"activity\", \"service\", \"statistic\"]\n",
        "        }\n",
        "    else:\n",
        "        return summary\n",
        "\n",
        "    missing_topics = []\n",
        "    for topic, keywords in key_topics.items():\n",
        "        if not any(kw.lower() in summary.lower() for kw in keywords):\n",
        "            missing_topics.append(topic)\n",
        "\n",
        "    if missing_topics:\n",
        "        note = f\"\\n\\nNote: This summary may not cover: {', '.join(missing_topics)}.\"\n",
        "        summary += note\n",
        "\n",
        "    return summary\n",
        "\n",
        "def safe_generate(prompt, max_length, min_length, tokenizer, model, max_retries=3):\n",
        "    tries = 0\n",
        "    while tries < max_retries:\n",
        "        try:\n",
        "            tokens = tokenizer.tokenize(prompt)\n",
        "            if len(tokens) > 1024:\n",
        "                tokens = tokens[:1024]\n",
        "                prompt = tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            model.to(device)\n",
        "\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                min_length=min_length,\n",
        "                do_sample=False,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "            result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in text generation: {e}\")\n",
        "            tries += 1\n",
        "            time.sleep(1)\n",
        "\n",
        "    return f\"Error generating summary after {max_retries} retries.\"\n",
        "\n",
        "def summarize_section(section_text, canonical_section, confidence, tokenizer, model):\n",
        "    if section_text.startswith(\"No '\") and section_text.endswith(\"' content found.\"):\n",
        "        return section_text\n",
        "\n",
        "    cleaned_section = preprocess_for_summarization(section_text)\n",
        "    chunks = chunk_text_intelligently(cleaned_section, max_words=300)\n",
        "\n",
        "    if len(cleaned_section.split()) < 150:\n",
        "        return cleaned_section\n",
        "\n",
        "    chunk_summaries = []\n",
        "    with ThreadPoolExecutor(max_workers=min(4, len(chunks))) as executor:\n",
        "        futures = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk_info = f\"(chunk {i+1}/{len(chunks)})\"\n",
        "            prompt = generate_section_specific_prompt(canonical_section, chunk, chunk_info)\n",
        "            chunk_word_count = len(chunk.split())\n",
        "            max_summary_length = max(150, min(300, chunk_word_count // 2))\n",
        "            min_summary_length = max(75, min(150, chunk_word_count // 4))\n",
        "\n",
        "            future = executor.submit(\n",
        "                safe_generate,\n",
        "                prompt,\n",
        "                max_length=max_summary_length,\n",
        "                min_length=min_summary_length,\n",
        "                tokenizer=tokenizer,\n",
        "                model=model\n",
        "            )\n",
        "            futures.append(future)\n",
        "\n",
        "        for future in futures:\n",
        "            chunk_summaries.append(future.result())\n",
        "\n",
        "    if len(chunk_summaries) == 1:\n",
        "        final_summary = chunk_summaries[0]\n",
        "    else:\n",
        "        combined_summary = \" \".join(chunk_summaries)\n",
        "        final_prompt = generate_final_summary_prompt(canonical_section, combined_summary, confidence)\n",
        "        final_summary = safe_generate(final_prompt, max_length=800, min_length=300, tokenizer=tokenizer, model=model)\n",
        "\n",
        "    final_summary = remove_redundancy(final_summary)\n",
        "    final_summary = verify_section_content(final_summary, canonical_section)\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "def analyze_document_structure(text):\n",
        "    patterns = {\n",
        "        'numbered': len(re.findall(r'(?:^|\\n)\\d+(?:\\.\\d+)*\\s+[A-Z]', text)),\n",
        "        'all_caps': len(re.findall(r'(?:^|\\n)[A-Z][A-Z\\s]{3,}(?:$|\\n)', text)),\n",
        "        'section_word': len(re.findall(r'(?:^|\\n)(?:SECTION|Section)\\s+[0-9A-Z]', text)),\n",
        "        'title_case': len(re.findall(r'(?:^|\\n)(?:[A-Z][a-z]+\\s+){1,3}(?:$|\\n)', text))\n",
        "    }\n",
        "    dominant_pattern = max(patterns.items(), key=lambda x: x[1])[0]\n",
        "    return {\n",
        "        'patterns': patterns,\n",
        "        'dominant_pattern': dominant_pattern,\n",
        "        'section_markers': identify_section_markers(text)\n",
        "    }\n",
        "\n",
        "def extract_additional_metadata(text):\n",
        "    metadata = {}\n",
        "    rfp_id_match = re.search(r'(?:RFP|Request for Proposal)[\\s#:]*([A-Z0-9-]+)', text, re.IGNORECASE)\n",
        "    if rfp_id_match:\n",
        "        metadata['rfp_id'] = rfp_id_match.group(1).strip()\n",
        "    due_date_match = re.search(r'(?:due|submission|deadline)(?:\\s+date)?(?:\\s*[-:]\\s*|\\s+is\\s+)([A-Za-z]+\\s+\\d{1,2},?\\s+\\d{4}|\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4})', text, re.IGNORECASE)\n",
        "    if due_date_match:\n",
        "        metadata['due_date'] = due_date_match.group(1).strip()\n",
        "    org_patterns = [\n",
        "        r'(?:issued|published|released)\\s+by(?:\\s+the)?\\s+([A-Z][A-Za-z\\s]+)(?:,|\\.|\\n)',\n",
        "        r'(?:^|\\n)([A-Z][A-Za-z\\s]+(?:Department|Agency|Corporation|Authority|Office|Bureau))'\n",
        "    ]\n",
        "    for pattern in org_patterns:\n",
        "        org_match = re.search(pattern, text, re.IGNORECASE)\n",
        "        if org_match:\n",
        "            metadata['issuing_organization'] = org_match.group(1).strip()\n",
        "            break\n",
        "    return metadata\n",
        "\n",
        "def process_section(canonical_section, full_text, tokenizer, model, markers):\n",
        "    logger.info(f\"Processing section: {canonical_section}\")\n",
        "    section_text, conf = extract_section_by_synonyms(full_text, canonical_section, markers)\n",
        "    if not section_text.startswith(\"No '\"):\n",
        "        summary = summarize_section(section_text, canonical_section, conf, tokenizer, model)\n",
        "        return canonical_section, section_text, summary, conf\n",
        "    else:\n",
        "        return canonical_section, \"\", section_text, 0.0\n",
        "\n",
        "def process_pdf(pdf_file, output_dir, tokenizer, model):\n",
        "    logger.info(f\"Processing PDF: {pdf_file}\")\n",
        "    raw_text = extract_text_from_pdf(pdf_file)\n",
        "    if not raw_text:\n",
        "        logger.error(f\"No text extracted from {pdf_file}\")\n",
        "        return None\n",
        "    cleaned_text = clean_text(raw_text)\n",
        "    doc_structure = analyze_document_structure(cleaned_text)\n",
        "    markers = doc_structure['section_markers']\n",
        "    metadata = extract_additional_metadata(cleaned_text)\n",
        "\n",
        "    target_sections = [\"scope of work\", \"background\"]\n",
        "\n",
        "    section_summaries = {}\n",
        "    section_confidences = {}\n",
        "\n",
        "    for sec in target_sections:\n",
        "        sec, sec_text, summary, conf = process_section(sec, cleaned_text, tokenizer, model, markers)\n",
        "        section_summaries[sec] = {\"text\": sec_text, \"summary\": summary}\n",
        "        section_confidences[sec] = conf\n",
        "        logger.info(f\"Section '{sec}' in {pdf_file} processed with confidence: {conf:.2f}\")\n",
        "\n",
        "    output_lines = []\n",
        "    output_lines.append(\"--- DOCUMENT METADATA ---\\n\")\n",
        "    if metadata:\n",
        "        for key, value in metadata.items():\n",
        "            output_lines.append(f\"{key.replace('_', ' ').title()}: {value}\\n\")\n",
        "    output_lines.append(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
        "\n",
        "    for sec in target_sections:\n",
        "        if sec in section_summaries and section_summaries[sec][\"summary\"]:\n",
        "            note = \" (Low confidence extraction)\" if section_confidences[sec] < 0.5 else \"\"\n",
        "            output_lines.append(f\"--- {sec.upper()} SUMMARY{note} ---\\n\")\n",
        "            output_lines.append(section_summaries[sec][\"summary\"] + \"\\n\")\n",
        "            output_lines.append(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "    pdf_filename = os.path.splitext(os.path.basename(pdf_file))[0]\n",
        "    output_file = os.path.join(output_dir, f\"{pdf_filename}_key_section_summaries.txt\")\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(output_lines)\n",
        "\n",
        "    logger.info(f\"Processing complete for {pdf_file}. Key section summaries saved to '{output_file}'\")\n",
        "    return output_file\n",
        "\n",
        "def process_document(file_path, target_sections=None):\n",
        "    if target_sections is None:\n",
        "        target_sections = [\"scope of work\", \"background\"]\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        raw_text = extract_text_from_pdf(file_path)\n",
        "        if not raw_text:\n",
        "            return {\"error\": \"Failed to extract text from PDF\"}\n",
        "\n",
        "        cleaned_text = clean_text(raw_text)\n",
        "\n",
        "        # Initialize the tokenizer and model - using t5-base for this example.\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "\n",
        "        structure_info = analyze_document_structure(cleaned_text)\n",
        "        metadata = extract_additional_metadata(cleaned_text)\n",
        "\n",
        "        results = {}\n",
        "        for section in target_sections:\n",
        "            section_text, confidence = extract_section_by_synonyms(\n",
        "                cleaned_text, section, structure_info[\"section_markers\"]\n",
        "            )\n",
        "\n",
        "            summary = summarize_section(section_text, section, confidence, tokenizer, model)\n",
        "            formatted_summary = summary  # You can call format_summary_with_structure(summary) if needed\n",
        "\n",
        "            results[section] = {\n",
        "                \"section_text\": section_text,\n",
        "                \"confidence\": confidence,\n",
        "                \"summary\": formatted_summary\n",
        "            }\n",
        "\n",
        "        results[\"metadata\"] = metadata\n",
        "        results[\"structure\"] = structure_info\n",
        "        results[\"processing_time\"] = time.time() - start_time\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing document {file_path}: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "def main():\n",
        "    # Hardcode the PDF file path here\n",
        "    pdf_file = \"/content/STAFF-8601 (2).pdf\"\n",
        "    output_dir = os.path.join(os.path.dirname(pdf_file), \"rfp_analysis_output\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # For this example, we use \"facebook/bart-large-cnn\"\n",
        "    model_name = \"facebook/bart-large-cnn\"\n",
        "\n",
        "    logger.info(\"Loading NLP model for summarization\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.to(device)\n",
        "\n",
        "    process_pdf(pdf_file, output_dir, tokenizer, model)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-1kzPDcOyZc"
      },
      "source": [
        "###matching score ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 649,
          "referenced_widgets": [
            "f27a3e508a0545ec92db2e3c02d969ae",
            "87e98c07c4414d2693ace0ff8cb48e6a",
            "be2f7a406b7a42b791a7bf2c78eac937",
            "03a3a2ff20904ef1bc79b2926a20a40d",
            "4eff6a7e818846e4b895d01e5da8f1b4",
            "3adff3bb544c4e33a8a1bc7990509e4c",
            "17a883162ec14d5c97b48f7c46c32b87",
            "a1fa7a3764cc4a19aee3b04a41423c91",
            "0a933823d7a3481094b29807b9957087",
            "f842d7b2d8ec45f29c5a123a5e4b3895",
            "9603d980004246de86d69adb62a65ded"
          ]
        },
        "id": "imq4RlYiOZW1",
        "outputId": "dd5a6109-4076-47cd-a93c-0f8fcc76944f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Scope of Work Summary:\n",
            "Refine and consolidate the following texts into a final, clear, and detailed summary of the 'Scope of Work' section. Focus exclusively on contractor responsibilities including provider sourcing, screening, reimbursement, insurance, licensure verification, travel/lodging management, and EHR documentation. The Kern Behavioral Health and Recovery Services (KernBHRS) administration office is located in Bakersfield, the county sear, in the San Joaquin Valley. KernBH RS is seeking a locum tenens agency capable of providing board certified or board eligible psychiatrists, psychiatric certified nurse practitioners and registered nurses with mental health experience. The Department expects to spend approximately $2,100,000 per fiscal year for these services among all providers. Services shall begin on July 1, 2023. Three Agreements will be negotiated between Kern BH RS and the prospective service provider. The final summary, formatted as a coherent paragraph followed by 4-7 bullet points of key responsibilities, can be found at: http://www.kernbhrs.org/RFPs/Kern-Behavioral-Health-and-Recovery-Services-(Kern BHRS-RFP) /Documents/KERN-Behavioural-health-and Recovery Services- (K Kern BhRS- RFP) /Documents/Bakersfield- Behavioral-Health and Recovery Services (KERN BH Health Recovery Services)\n",
            "\n",
            "Note: This summary may not cover: licensure.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f27a3e508a0545ec92db2e3c02d969ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87e98c07c4414d2693ace0ff8cb48e6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be2f7a406b7a42b791a7bf2c78eac937",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03a3a2ff20904ef1bc79b2926a20a40d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4eff6a7e818846e4b895d01e5da8f1b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3adff3bb544c4e33a8a1bc7990509e4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17a883162ec14d5c97b48f7c46c32b87",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1fa7a3764cc4a19aee3b04a41423c91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a933823d7a3481094b29807b9957087",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f842d7b2d8ec45f29c5a123a5e4b3895",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9603d980004246de86d69adb62a65ded",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Similarity Score: 0.30\n",
            "Partial match: Your experience has some relevant elements.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download required NLTK resources if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Function to extract the \"Scope of Work\" summary from the output file\n",
        "def extract_scope_summary(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "\n",
        "        pattern = r\"--- SCOPE OF WORK SUMMARY ---\\s*(.*?)(?:\\n[-]{10,}|\\Z)\"\n",
        "        match = re.search(pattern, content, re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        print(\"Scope of Work summary not found.\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase and remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text.lower())\n",
        "    # Remove URLs and file paths\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+|\\S+\\.com\\S*|\\S+\\.org\\S*|\\S*\\.pdf', ' ', text)\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "# Function to split text into manageable chunks\n",
        "def split_into_chunks(text, max_length=512):\n",
        "    if len(text) < max_length:\n",
        "        return [text]\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) < max_length:\n",
        "            current_chunk += \" \" + sentence\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    # Fallback to splitting by words if sentences are too long\n",
        "    if not chunks:\n",
        "        words = text.split()\n",
        "        current_chunk = \"\"\n",
        "        for word in words:\n",
        "            if len(current_chunk) + len(word) + 1 < max_length:\n",
        "                current_chunk += \" \" + word\n",
        "            else:\n",
        "                chunks.append(current_chunk.strip())\n",
        "                current_chunk = word\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks or [text[:max_length]]\n",
        "\n",
        "# Function to compute semantic similarity using sentence transformers\n",
        "def compute_similarity(text1, text2, model_name='all-MiniLM-L6-v2'):\n",
        "    try:\n",
        "        # Clean and preprocess texts\n",
        "        clean_text1 = preprocess_text(text1)\n",
        "        clean_text2 = preprocess_text(text2)\n",
        "\n",
        "        if not clean_text1 or not clean_text2:\n",
        "            print(\"Warning: One or both texts are empty after preprocessing\")\n",
        "            return 0.0\n",
        "\n",
        "        # Load model and split texts into chunks\n",
        "        model = SentenceTransformer(model_name)\n",
        "        chunks1 = split_into_chunks(clean_text1)\n",
        "        chunks2 = split_into_chunks(clean_text2)\n",
        "\n",
        "        # Encode all chunks\n",
        "        embeddings1 = model.encode(chunks1, convert_to_tensor=True)\n",
        "        embeddings2 = model.encode(chunks2, convert_to_tensor=True)\n",
        "\n",
        "        # Calculate best matches for each chunk\n",
        "        cosine_scores = []\n",
        "        for emb1 in embeddings1:\n",
        "            chunk_sims = [util.pytorch_cos_sim(emb1, emb2).item() for emb2 in embeddings2]\n",
        "            cosine_scores.append(max(chunk_sims) if chunk_sims else 0)\n",
        "\n",
        "        # Average the similarities\n",
        "        similarity = sum(cosine_scores) / len(cosine_scores) if cosine_scores else 0.0\n",
        "\n",
        "        # Calibrate the score to provide better differentiation\n",
        "        calibrated_similarity = (similarity - 0.3) * 1.4\n",
        "        return max(0.0, min(1.0, calibrated_similarity))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing similarity: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "# Function to provide feedback based on similarity score\n",
        "def provide_feedback(score):\n",
        "    if score >= 0.75:\n",
        "        return \"Strong match: Your experience closely aligns with the scope requirements.\"\n",
        "    elif score >= 0.5:\n",
        "        return \"Good match: Your experience has significant overlap with the requirements.\"\n",
        "    elif score >= 0.25:\n",
        "        return \"Partial match: Your experience has some relevant elements.\"\n",
        "    else:\n",
        "        return \"Limited match: Consider highlighting relevant transferable skills.\"\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the path to the output file\n",
        "    output_file_path = os.path.join(\"rfp_analysis_output\", \"STAFF-8601 (2)_key_section_summaries.txt\")\n",
        "\n",
        "    # Extract the Scope of Work summary\n",
        "    scope_summary = extract_scope_summary(output_file_path)\n",
        "    if not scope_summary:\n",
        "        print(\"Could not extract the Scope of Work summary.\")\n",
        "        exit(1)\n",
        "\n",
        "    print(\"Extracted Scope of Work Summary:\")\n",
        "    print(scope_summary)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Get user input for their experience\n",
        "    user_experience = input(\"Enter your experience description: \").strip()\n",
        "    if not user_experience:\n",
        "        print(\"No experience input provided.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Calculate similarity and provide feedback\n",
        "    similarity_score = compute_similarity(scope_summary, user_experience)\n",
        "    print(f\"\\nSimilarity Score: {similarity_score:.2f}\")\n",
        "    print(provide_feedback(similarity_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQPTDg12Ov0z"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpuvvOKAO_gz"
      },
      "source": [
        "###requirements.txt###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IX1l2O9uPEJZ",
        "outputId": "67990f49-7a8a-41fb-fe2d-8a0e6a0e496b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'PyMuPDF\\ntorch\\ntransformers\\nnltk\\nscikit-learn\\nnumpy\\nsentence-transformers'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''PyMuPDF\n",
        "torch\n",
        "transformers\n",
        "nltk\n",
        "scikit-learn\n",
        "numpy\n",
        "sentence-transformers'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2bf64063ca1d433b82ad7d910419db99",
            "d2708c8e28f040d0b973efb02ce30d85",
            "dfc5f8cb3fe148afab31a18ca20cecd5",
            "4e0116ea4d124511a5fdf6fe3de4172b",
            "7489cb85083149e9bcfc4e330e90a4e2",
            "193364ff39fe4d6c99f30c21d18e768d",
            "094d1c3ceb744d16bb77b5a215485730",
            "c417da0c236e4bd9927dd43af55fbb69",
            "9ca9625864394de48e5298a8323e0916",
            "091ae82b5a5d4ffa9b32de924ec99a9c",
            "87dff64600ed41ad8e2959db23bc425b",
            "d63ef2de3747429cb88781c6f6b5d95c",
            "899907fd7a1a4725b8be1d356ff936d8",
            "61a2547bbf0b449eb5a58e0d305aa48e"
          ]
        },
        "id": "yRYzHkdny8Ig",
        "outputId": "4ba53000-4356-43ee-a9b8-d8674aa5ce64"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter or paste the document summary text (press Ctrl+D or Ctrl+Z on a new line when finished):\n",
            "\n",
            "Choose model quality (better quality = slower):\n",
            "1. Fast (lower quality)\n",
            "2. Balanced (medium quality)\n",
            "3. High quality (slower)\n",
            "\n",
            "Generating answer using google/flan-t5-xxl...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bf64063ca1d433b82ad7d910419db99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2708c8e28f040d0b973efb02ce30d85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "(…)a5b18a05535c9e14c7a355904270e15b0945ea86:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfc5f8cb3fe148afab31a18ca20cecd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e0116ea4d124511a5fdf6fe3de4172b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7489cb85083149e9bcfc4e330e90a4e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "193364ff39fe4d6c99f30c21d18e768d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "094d1c3ceb744d16bb77b5a215485730",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c417da0c236e4bd9927dd43af55fbb69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00005.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ca9625864394de48e5298a8323e0916",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00005.safetensors:   0%|          | 0.00/9.60G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "091ae82b5a5d4ffa9b32de924ec99a9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00005.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87dff64600ed41ad8e2959db23bc425b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00005.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d63ef2de3747429cb88781c6f6b5d95c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00005-of-00005.safetensors:   0%|          | 0.00/6.06G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "899907fd7a1a4725b8be1d356ff936d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61a2547bbf0b449eb5a58e0d305aa48e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.92` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-11-bb762891b197>\", line 138, in <cell line: 0>\n",
            "    run_scope_analyzer()\n",
            "  File \"<ipython-input-11-bb762891b197>\", line 122, in run_scope_analyzer\n",
            "    answer, time_taken = generate_answer_with_llm(summary_text, question, model_name)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-bb762891b197>\", line 60, in generate_answer_with_llm\n",
            "    outputs = model.generate(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2254, in generate\n",
            "    result = self._beam_search(\n",
            "             ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3463, in _beam_search\n",
            "    outputs = self(**model_inputs, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 1893, in forward\n",
            "    decoder_outputs = self.decoder(\n",
            "                      ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 1124, in forward\n",
            "    layer_outputs = layer_module(\n",
            "                    ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 725, in forward\n",
            "    hidden_states = self.layer[-1](hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 339, in forward\n",
            "    forwarded_states = self.DenseReluDense(forwarded_states)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\", line 307, in forward\n",
            "    hidden_gelu = self.act(self.wi_0(hidden_states))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1688, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "                  ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 170, in findsource\n",
            "    file = getsourcefile(object) or getfile(object)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 988, in getmodule\n",
            "    if ismodule(module) and hasattr(module, '__file__'):\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-bb762891b197>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mrun_scope_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-bb762891b197>\u001b[0m in \u001b[0;36mrun_scope_analyzer\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nGenerating answer using {model_name}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_answer_with_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-bb762891b197>\u001b[0m in \u001b[0;36mgenerate_answer_with_llm\u001b[0;34m(summary_text, question, model_name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Generate output with improved parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m             \u001b[0;31m# 13. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2254\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2255\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unchanged original behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1892\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1893\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1894\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1125\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mhidden_gelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mhidden_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "# Continuation Code: Generative Answer Integration\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "def generate_answer(summarized_text, user_question, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Generates an answer to the user question using the summarized text as context.\n",
        "\n",
        "    Parameters:\n",
        "    - summarized_text: The document summary (e.g., Scope of Work summary).\n",
        "    - user_question: The question provided by the user.\n",
        "    - tokenizer: The tokenizer for the generative model.\n",
        "    - model: The generative model.\n",
        "\n",
        "    Returns:\n",
        "    - answer: The generated answer text.\n",
        "    \"\"\"\n",
        "    # Build a prompt that combines the summary and user question.\n",
        "    prompt = (\n",
        "        f\"Based on the following document summary:\\n\\n{summarized_text}\\n\\n\"\n",
        "        f\"Please answer the following question:\\n{user_question}\\n\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # Use safe_generate (from your previous code) to produce the answer.\n",
        "    answer = safe_generate(prompt, max_length=300, min_length=100, tokenizer=tokenizer, model=model)\n",
        "    return answer\n",
        "\n",
        "def run_generative_qa():\n",
        "    # Specify the path to your PDF file (make sure it matches your setup)\n",
        "    file_path = \"/content/STAFF-8601 (2).pdf\"\n",
        "\n",
        "    # Process the document using your existing function to obtain section summaries\n",
        "    results = process_document(file_path)\n",
        "\n",
        "    # For this example, we'll use the \"Scope of Work\" summary as context.\n",
        "    summary_text = results.get(\"scope of work\", {}).get(\"summary\", \"\")\n",
        "    if not summary_text:\n",
        "        print(\"No 'Scope of Work' summary available. Please check your document processing.\")\n",
        "        return\n",
        "\n",
        "    print(\"Document Summary (Scope of Work):\\n\")\n",
        "    print(summary_text)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Get the user's question.\n",
        "    user_question = input(\"Enter your question regarding the document: \").strip()\n",
        "    if not user_question:\n",
        "        print(\"No question provided. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Load a generative model (using t5-base as an example) for answering queries.\n",
        "    model_name = \"t5-base\"\n",
        "    tokenizer_gen = AutoTokenizer.from_pretrained(model_name)\n",
        "    model_gen = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model_gen.to(device)\n",
        "\n",
        "    # Generate an answer using the summarized text and user query.\n",
        "    answer = generate_answer(summary_text, user_question, tokenizer_gen, model_gen)\n",
        "    print(\"\\nGenerated Answer:\\n\")\n",
        "    print(answer)\n",
        "\n",
        "# Run the generative QA function\n",
        "run_generative_qa()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t47iy_pZzAEm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNzWde9Qx092fAsaDp8Rpc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}